{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ADS NASA Query Analysis Notebook\n",
        "\n",
        "This notebook performs queries to the NASA ADS (Astrophysics Data System) using keywords extracted from our wordcloud analysis.\n",
        "\n",
        "## Features:\n",
        "- Extract top keywords from wordcloud analysis\n",
        "- Query ADS to count publications before retrieval\n",
        "- Smart query planning to avoid overwhelming results\n",
        "- Retrieve and analyze relevant publications\n",
        "\n",
        "## Workflow:\n",
        "1. **Extract Keywords**: Get top words from titles and abstracts\n",
        "2. **Count Check**: Query ADS to see how many publications match\n",
        "3. **Query Planning**: Adjust search strategy based on result count\n",
        "4. **Data Retrieval**: Fetch publication data if reasonable\n",
        "5. **Analysis**: Process and analyze the retrieved publications\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Libraries imported successfully\n",
            "‚úÖ Ready to start ADS analysis!\n"
          ]
        }
      ],
      "source": [
        "# Setup and imports\n",
        "import sys\n",
        "import os\n",
        "import json\n",
        "from pprint import pprint\n",
        "\n",
        "# Add the src directory to the path\n",
        "sys.path.append('../src')\n",
        "\n",
        "# Import our custom modules\n",
        "from wordcloud_utils import extract_top_words_from_json_files\n",
        "from ads_parser import (\n",
        "    test_ads_connection,\n",
        "    search_papers_by_keywords,\n",
        "    process_search_results,\n",
        "    search_and_process_papers\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Libraries imported successfully\")\n",
        "print(\"‚úÖ Ready to start ADS analysis!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Test ADS Connection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Testing ADS API connection...\n",
            "üîç Testing ADS API connection...\n",
            "‚úÖ ADS API connection successful!\n",
            "   Found 1103 total results\n",
            "   Retrieved 1 documents\n",
            "\n",
            "‚úÖ ADS connection successful! Ready to proceed.\n"
          ]
        }
      ],
      "source": [
        "# Test ADS API connection\n",
        "print(\"üöÄ Testing ADS API connection...\")\n",
        "connection_ok = test_ads_connection()\n",
        "\n",
        "if connection_ok:\n",
        "    print(\"\\n‚úÖ ADS connection successful! Ready to proceed.\")\n",
        "else:\n",
        "    print(\"\\n‚ùå ADS connection failed. Please check your API token in .env file.\")\n",
        "    print(\"You can get an ADS API token from: https://ui.adsabs.harvard.edu/user/settings/token\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Extract Keywords from Wordcloud Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Extracting keywords from wordcloud analysis...\n",
            "\n",
            "üìù Extracted keyword sets:\n",
            "Top 4 unique keywords: 6 words\n",
            "  ['period', 'eclipsing', 'photometric', 'contact', 'light', 'binary']\n",
            "\n",
            "Top 5 unique keywords: 8 words\n",
            "  ['period', 'eclipsing', 'photometric', 'contact', 'uma', 'light', 'mass', 'binary']\n",
            "\n",
            "Top 10 unique keywords: 13 words\n",
            "  ['binaries', 'system', 'type', 'period', 'curves', 'eclipsing', 'photometric', 'contact', 'uma', 'light', 'mass', 'orbital', 'binary']\n",
            "\n",
            "Top 15 unique keywords: 20 words\n",
            "  ['binaries', 'system', 'type', 'curve', 'photometric', 'investigation', 'orbital', 'period', 'curves', 'parameters', 'contact', 'uma', 'binary', 'short', 'component', 'ratio', 'systems', 'eclipsing', 'light', 'mass']\n"
          ]
        }
      ],
      "source": [
        "# Extract top keywords from our wordcloud analysis\n",
        "titles_freq_file = '../wordclouds/titles_word_frequencies.json'\n",
        "abstracts_freq_file = '../wordclouds/abstracts_word_frequencies.json'\n",
        "\n",
        "# Check if the wordcloud files exist\n",
        "if not os.path.exists(titles_freq_file) or not os.path.exists(abstracts_freq_file):\n",
        "    print(\"‚ùå Wordcloud frequency files not found!\")\n",
        "    print(\"Please run the wordcloud_analysis.ipynb notebook first to generate the frequency data.\")\n",
        "    print(f\"Looking for:\")\n",
        "    print(f\"  - {titles_freq_file}\")\n",
        "    print(f\"  - {abstracts_freq_file}\")\n",
        "else:\n",
        "    # Extract different sets of keywords for testing\n",
        "    print(\"üìä Extracting keywords from wordcloud analysis...\")\n",
        "    \n",
        "    # Start with a small set for testing\n",
        "    top_4_keywords = extract_top_words_from_json_files(titles_freq_file, abstracts_freq_file, 4)\n",
        "    top_5_keywords = extract_top_words_from_json_files(titles_freq_file, abstracts_freq_file, 5)\n",
        "    top_10_keywords = extract_top_words_from_json_files(titles_freq_file, abstracts_freq_file, 10)\n",
        "    top_15_keywords = extract_top_words_from_json_files(titles_freq_file, abstracts_freq_file, 15)\n",
        "    \n",
        "    print(f\"\\nüìù Extracted keyword sets:\")\n",
        "    print(f\"Top 4 unique keywords: {len(top_4_keywords)} words\")\n",
        "    print(f\"  {top_4_keywords}\")\n",
        "    print(f\"\\nTop 5 unique keywords: {len(top_5_keywords)} words\")\n",
        "    print(f\"  {top_5_keywords}\")\n",
        "    print(f\"\\nTop 10 unique keywords: {len(top_10_keywords)} words\")\n",
        "    print(f\"  {top_10_keywords}\")\n",
        "    print(f\"\\nTop 15 unique keywords: {len(top_15_keywords)} words\")\n",
        "    print(f\"  {top_15_keywords}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Count Publications Before Retrieval\n",
        "\n",
        "Before retrieving any data, let's see how many publications each keyword set would return.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "PUBLICATION COUNT ANALYSIS\n",
            "============================================================\n",
            "\n",
            "üîç Counting publications for Top 4 keywords (full text search)...\n",
            "Keywords: ['period', 'eclipsing', 'photometric', 'contact', 'light', 'binary']\n",
            "Search fields: full\n",
            "üîç Searching for: full:period AND full:eclipsing AND full:photometric AND full:contact AND full:light AND full:binary\n",
            "‚úÖ Found 9425 papers, retrieved 1\n",
            "üìä Total publications found: 9,425\n",
            "\n",
            "üîç Counting publications for Top 5 keywords (full text search)...\n",
            "Keywords: ['period', 'eclipsing', 'photometric', 'contact', 'uma', 'light', 'mass', 'binary']\n",
            "Search fields: full\n",
            "üîç Searching for: full:period AND full:eclipsing AND full:photometric AND full:contact AND full:uma AND full:light AND full:mass AND full:binary\n",
            "‚úÖ Found 3402 papers, retrieved 1\n",
            "üìä Total publications found: 3,402\n",
            "\n",
            "üîç Counting publications for Top 10 keywords (full text search)...\n",
            "Keywords: ['binaries', 'system', 'type', 'period', 'curves', 'eclipsing', 'photometric', 'contact', 'uma', 'light', 'mass', 'orbital', 'binary']\n",
            "Search fields: full\n",
            "üîç Searching for: full:binaries AND full:system AND full:type AND full:period AND full:curves AND full:eclipsing AND full:photometric AND full:contact AND full:uma AND full:light AND full:mass AND full:orbital AND full:binary\n",
            "‚úÖ Found 3066 papers, retrieved 1\n",
            "üìä Total publications found: 3,066\n",
            "\n",
            "üîç Counting publications for Top 15 keywords (full text search)...\n",
            "Keywords: ['binaries', 'system', 'type', 'curve', 'photometric', 'investigation', 'orbital', 'period', 'curves', 'parameters', 'contact', 'uma', 'binary', 'short', 'component', 'ratio', 'systems', 'eclipsing', 'light', 'mass']\n",
            "Search fields: full\n",
            "üîç Searching for: full:binaries AND full:system AND full:type AND full:curve AND full:photometric AND full:investigation AND full:orbital AND full:period AND full:curves AND full:parameters AND full:contact AND full:uma AND full:binary AND full:short AND full:component AND full:ratio AND full:systems AND full:eclipsing AND full:light AND full:mass\n",
            "‚úÖ Found 1650 papers, retrieved 1\n",
            "üìä Total publications found: 1,650\n",
            "\n",
            "============================================================\n",
            "SUMMARY OF PUBLICATION COUNTS\n",
            "============================================================\n",
            "top4_full           : 9,425 publications\n",
            "top5_full           : 3,402 publications\n",
            "top10_full          : 3,066 publications\n",
            "top15_full          : 1,650 publications\n",
            "\n",
            "üí° RECOMMENDATIONS:\n",
            "‚ö†Ô∏è  top4_full: 9,425 publications - Large but feasible with pagination\n",
            "‚ö†Ô∏è  top5_full: 3,402 publications - Large but feasible with pagination\n",
            "‚ö†Ô∏è  top10_full: 3,066 publications - Large but feasible with pagination\n",
            "‚ö†Ô∏è  top15_full: 1,650 publications - Large but feasible with pagination\n"
          ]
        }
      ],
      "source": [
        "def count_publications_for_keywords(keywords, description=\"\"):\n",
        "    \"\"\"\n",
        "    Count how many publications match the given keywords in the full text without retrieving the full data.\n",
        "    \"\"\"\n",
        "    search_fields = \"full\"\n",
        "    print(f\"\\nüîç Counting publications for {description}...\")\n",
        "    print(f\"Keywords: {keywords}\")\n",
        "    print(f\"Search fields: {search_fields}\")\n",
        "    \n",
        "    # Use max_results=1 to minimize data transfer while getting the count\n",
        "    results = search_papers_by_keywords(keywords, search_fields=search_fields, max_results=1)\n",
        "    \n",
        "    if results and \"response\" in results:\n",
        "        total_count = results[\"response\"].get(\"numFound\", 0)\n",
        "        print(f\"üìä Total publications found: {total_count:,}\")\n",
        "        return total_count\n",
        "    else:\n",
        "        print(\"‚ùå Failed to get publication count\")\n",
        "        return 0\n",
        "\n",
        "# Count publications for different keyword sets using only 'full' search\n",
        "if 'top_5_keywords' in locals():\n",
        "    counts = {}\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"PUBLICATION COUNT ANALYSIS\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Top 4 keywords - full text search only\n",
        "    counts['top4_full'] = count_publications_for_keywords(\n",
        "        top_4_keywords, \"Top 4 keywords (full text search)\"\n",
        "    )\n",
        "    \n",
        "    # Top 5 keywords - full text search only\n",
        "    counts['top5_full'] = count_publications_for_keywords(\n",
        "        top_5_keywords, \"Top 5 keywords (full text search)\"\n",
        "    )\n",
        "    \n",
        "    # Top 10 keywords - full text search only\n",
        "    counts['top10_full'] = count_publications_for_keywords(\n",
        "        top_10_keywords, \"Top 10 keywords (full text search)\"\n",
        "    )\n",
        "    \n",
        "    # Top 15 keywords - full text search only\n",
        "    counts['top15_full'] = count_publications_for_keywords(\n",
        "        top_15_keywords, \"Top 15 keywords (full text search)\"\n",
        "    )\n",
        "    \n",
        "    # Show summary\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"SUMMARY OF PUBLICATION COUNTS\")\n",
        "    print(\"=\"*60)\n",
        "    for strategy, count in counts.items():\n",
        "        print(f\"{strategy:20s}: {count:,} publications\")\n",
        "        \n",
        "    # Recommendations\n",
        "    print(\"\\nüí° RECOMMENDATIONS:\")\n",
        "    manageable_threshold = 1000\n",
        "    large_threshold = 10000\n",
        "    \n",
        "    for strategy, count in counts.items():\n",
        "        if count <= manageable_threshold:\n",
        "            print(f\"‚úÖ {strategy}: {count:,} publications - Manageable for full retrieval\")\n",
        "        elif count <= large_threshold:\n",
        "            print(f\"‚ö†Ô∏è  {strategy}: {count:,} publications - Large but feasible with pagination\")\n",
        "        else:\n",
        "            print(f\"‚ùå {strategy}: {count:,} publications - Too many, consider refining keywords\")\n",
        "else:\n",
        "    print(\"‚ùå Keywords not available. Please run the previous cell first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Search for Bibcodes Only\n",
        "\n",
        "Now let's search for papers using 15 keywords and retrieve only the bibcodes using the batch query function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üéØ BIBCODE SEARCH WITH TOP 5 KEYWORDS\n",
            "==================================================\n",
            "Keywords: ['period', 'eclipsing', 'photometric', 'contact', 'uma', 'light', 'mass', 'binary']\n",
            "Search field: full\n",
            "Max results: 3402\n",
            "üîç Searching for bibcodes with query: full:period AND full:eclipsing AND full:photometric AND full:contact AND full:uma AND full:light AND full:mass AND full:binary\n",
            "üìä Getting total count first...\n",
            "‚úÖ Found 3,402 total papers\n",
            "üìÑ Will need 2 requests to get all bibcodes\n",
            "üì• Request 1/2: Getting bibcodes 1-2,000\n",
            "   ‚úÖ Retrieved 2000 bibcodes\n",
            "   üîÑ API requests remaining: 4962\n",
            "üì• Request 2/2: Getting bibcodes 2,001-3,402\n",
            "   ‚úÖ Retrieved 1402 bibcodes\n",
            "   üîÑ API requests remaining: 4961\n",
            "\n",
            "üéØ FINAL RESULTS:\n",
            "   Total papers found: 3,402\n",
            "   Total bibcodes retrieved: 3,402\n",
            "   API requests used: 2\n",
            "\n",
            "üìã BIBCODE SEARCH RESULTS:\n",
            "Total bibcodes retrieved: 3402\n",
            "\n",
            "First 10 bibcodes:\n",
            "   1. 2025NewA..12102445Y\n",
            "   2. 2025NewA..11902392H\n",
            "   3. 2025NewA..11902418Z\n",
            "   4. 2025RAA....25i5018W\n",
            "   5. 2025RAA....25h5002B\n",
            "   6. 2025NewA..11802374N\n",
            "   7. 2025AJ....170..101G\n",
            "   8. 2025A&A...700A..29C\n",
            "   9. 2025AJ....170..126S\n",
            "  10. 2025arXiv250713515S\n",
            "  ... and 3392 more\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from ads_parser import ADS_API_TOKEN, ADS_API_BASE_URL\n",
        "import time\n",
        "\n",
        "def search_all_bibcodes(keywords, search_fields=\"full\"):\n",
        "    \"\"\"\n",
        "    Search for papers and return ALL bibcodes using pagination.\n",
        "    Handles the 2000 per request limit automatically.\n",
        "    \"\"\"\n",
        "    if not ADS_API_TOKEN:\n",
        "        print(\"‚ùå Error: ADS_API_TOKEN not found in environment variables\")\n",
        "        return []\n",
        "    \n",
        "    headers = {\"Authorization\": f\"Bearer {ADS_API_TOKEN}\"}\n",
        "    \n",
        "    # Build query based on search fields\n",
        "    if search_fields == \"title\":\n",
        "        query_parts = [f\"title:{keyword}\" for keyword in keywords]\n",
        "    elif search_fields == \"abs\":\n",
        "        query_parts = [f\"abs:{keyword}\" for keyword in keywords]\n",
        "    elif search_fields == \"full\":\n",
        "        query_parts = [f\"full:{keyword}\" for keyword in keywords]\n",
        "    elif search_fields == \"title,abs\":\n",
        "        title_parts = [f\"title:{keyword}\" for keyword in keywords]\n",
        "        abs_parts = [f\"abs:{keyword}\" for keyword in keywords]\n",
        "        query_parts = title_parts + abs_parts\n",
        "    else:\n",
        "        print(f\"‚ùå Invalid search_fields: {search_fields}\")\n",
        "        return []\n",
        "    \n",
        "    # Join keywords with AND operator\n",
        "    query = \" AND \".join(query_parts)\n",
        "    \n",
        "    # First request to get total count\n",
        "    initial_params = {\n",
        "        \"q\": query,\n",
        "        \"fl\": \"bibcode\",\n",
        "        \"rows\": 1,  # Just get count first\n",
        "        \"sort\": \"date desc\"\n",
        "    }\n",
        "    \n",
        "    try:\n",
        "        print(f\"üîç Searching for bibcodes with query: {query}\")\n",
        "        print(f\"üìä Getting total count first...\")\n",
        "        \n",
        "        response = requests.get(\n",
        "            f\"{ADS_API_BASE_URL}/search/query\",\n",
        "            headers=headers,\n",
        "            params=initial_params,\n",
        "            timeout=30\n",
        "        )\n",
        "        \n",
        "        if response.status_code != 200:\n",
        "            print(f\"‚ùå Initial request failed with status {response.status_code}\")\n",
        "            return []\n",
        "        \n",
        "        data = response.json()\n",
        "        total_found = data.get(\"response\", {}).get(\"numFound\", 0)\n",
        "        \n",
        "        if total_found == 0:\n",
        "            print(\"‚ùå No papers found for this query\")\n",
        "            return []\n",
        "        \n",
        "        print(f\"‚úÖ Found {total_found:,} total papers\")\n",
        "        \n",
        "        # Calculate pagination\n",
        "        max_per_request = 2000\n",
        "        requests_needed = (total_found + max_per_request - 1) // max_per_request\n",
        "        \n",
        "        print(f\"üìÑ Will need {requests_needed} requests to get all bibcodes\")\n",
        "        \n",
        "        all_bibcodes = []\n",
        "        \n",
        "        # Get all bibcodes with pagination\n",
        "        for i in range(requests_needed):\n",
        "            start = i * max_per_request\n",
        "            remaining = total_found - start\n",
        "            rows = min(max_per_request, remaining)\n",
        "            \n",
        "            print(f\"üì• Request {i+1}/{requests_needed}: Getting bibcodes {start+1:,}-{start+rows:,}\")\n",
        "            \n",
        "            params = {\n",
        "                \"q\": query,\n",
        "                \"fl\": \"bibcode\",\n",
        "                \"rows\": rows,\n",
        "                \"start\": start,\n",
        "                \"sort\": \"date desc\"\n",
        "            }\n",
        "            \n",
        "            response = requests.get(\n",
        "                f\"{ADS_API_BASE_URL}/search/query\",\n",
        "                headers=headers,\n",
        "                params=params,\n",
        "                timeout=30\n",
        "            )\n",
        "            \n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                docs = data.get(\"response\", {}).get(\"docs\", [])\n",
        "                batch_bibcodes = [doc.get(\"bibcode\") for doc in docs if doc.get(\"bibcode\")]\n",
        "                all_bibcodes.extend(batch_bibcodes)\n",
        "                \n",
        "                print(f\"   ‚úÖ Retrieved {len(batch_bibcodes)} bibcodes\")\n",
        "                \n",
        "                # Check rate limit\n",
        "                if 'X-RateLimit-Remaining' in response.headers:\n",
        "                    remaining_requests = response.headers['X-RateLimit-Remaining']\n",
        "                    print(f\"   üîÑ API requests remaining: {remaining_requests}\")\n",
        "                \n",
        "                # Small delay between requests to be nice to the API\n",
        "                if i < requests_needed - 1:  # Don't delay after last request\n",
        "                    time.sleep(0.5)\n",
        "                    \n",
        "            else:\n",
        "                print(f\"   ‚ùå Request {i+1} failed with status {response.status_code}\")\n",
        "                break\n",
        "        \n",
        "        print(f\"\\nüéØ FINAL RESULTS:\")\n",
        "        print(f\"   Total papers found: {total_found:,}\")\n",
        "        print(f\"   Total bibcodes retrieved: {len(all_bibcodes):,}\")\n",
        "        print(f\"   API requests used: {min(i+1, requests_needed)}\")\n",
        "        \n",
        "        return all_bibcodes\n",
        "        \n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"‚ùå Request failed: {e}\")\n",
        "        return []\n",
        "\n",
        "# Use top 5 keywords for the search, and set max_results to the count from top5_full\n",
        "print(\"üéØ BIBCODE SEARCH WITH TOP 5 KEYWORDS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "CHOSEN_KEYWORDS = top_5_keywords  # Using top 5 keywords\n",
        "CHOSEN_SEARCH_FIELD = \"full\"      # Full text search\n",
        "\n",
        "# Use the count from counts['top5_full'] for max_results\n",
        "MAX_RESULTS = counts['top5_full'] if 'top5_full' in counts else 2000\n",
        "\n",
        "print(f\"Keywords: {CHOSEN_KEYWORDS}\")\n",
        "print(f\"Search field: {CHOSEN_SEARCH_FIELD}\")\n",
        "print(f\"Max results: {MAX_RESULTS}\")\n",
        "\n",
        "# Search for bibcodes\n",
        "found_bibcodes = search_all_bibcodes(CHOSEN_KEYWORDS, CHOSEN_SEARCH_FIELD)\n",
        "\n",
        "print(f\"\\nüìã BIBCODE SEARCH RESULTS:\")\n",
        "print(f\"Total bibcodes retrieved: {len(found_bibcodes)}\")\n",
        "\n",
        "if found_bibcodes:\n",
        "    print(f\"\\nFirst 10 bibcodes:\")\n",
        "    for i, bibcode in enumerate(found_bibcodes[:10]):\n",
        "        print(f\"  {i+1:2d}. {bibcode}\")\n",
        "    \n",
        "    if len(found_bibcodes) > 10:\n",
        "        print(f\"  ... and {len(found_bibcodes) - 10} more\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4b: Search with Top 4 Keywords\n",
        "\n",
        "Let's also perform a search with the top 4 keywords to see how the results differ with fewer, more specific terms.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üéØ BIBCODE SEARCH WITH TOP 4 KEYWORDS - ALL RESULTS\n",
            "============================================================\n",
            "Keywords: ['period', 'eclipsing', 'photometric', 'contact', 'light', 'binary']\n",
            "Search field: full\n",
            "Getting ALL bibcodes (automatic pagination)\n",
            "üîç Searching for bibcodes with query: full:period AND full:eclipsing AND full:photometric AND full:contact AND full:light AND full:binary\n",
            "üìä Getting total count first...\n",
            "‚úÖ Found 9,425 total papers\n",
            "üìÑ Will need 5 requests to get all bibcodes\n",
            "üì• Request 1/5: Getting bibcodes 1-2,000\n",
            "   ‚úÖ Retrieved 2000 bibcodes\n",
            "   üîÑ API requests remaining: 4955\n",
            "üì• Request 2/5: Getting bibcodes 2,001-4,000\n",
            "   ‚úÖ Retrieved 2000 bibcodes\n",
            "   üîÑ API requests remaining: 4954\n",
            "üì• Request 3/5: Getting bibcodes 4,001-6,000\n",
            "   ‚úÖ Retrieved 2000 bibcodes\n",
            "   üîÑ API requests remaining: 4953\n",
            "üì• Request 4/5: Getting bibcodes 6,001-8,000\n",
            "   ‚úÖ Retrieved 2000 bibcodes\n",
            "   üîÑ API requests remaining: 4952\n",
            "üì• Request 5/5: Getting bibcodes 8,001-9,425\n",
            "   ‚úÖ Retrieved 1425 bibcodes\n",
            "   üîÑ API requests remaining: 4951\n",
            "\n",
            "üéØ FINAL RESULTS:\n",
            "   Total papers found: 9,425\n",
            "   Total bibcodes retrieved: 9,425\n",
            "   API requests used: 5\n",
            "\n",
            "üìã TOP 4 KEYWORDS SEARCH RESULTS:\n",
            "Total bibcodes retrieved: 9,425\n",
            "\n",
            "First 10 bibcodes:\n",
            "   1. 2025NewA..12102445Y\n",
            "   2. 2025AcAau.235..251G\n",
            "   3. 2025NewA..11902392H\n",
            "   4. 2025NewA..11902418Z\n",
            "   5. 2025NewA..11902406W\n",
            "   6. 2025ChEnJ.51964904G\n",
            "   7. 2025RAA....25i5018W\n",
            "   8. 2025arXiv250800369J\n",
            "   9. 2025RAA....25h5002B\n",
            "  10. 2025NewA..11802374N\n",
            "  ... and 9,415 more\n",
            "\n",
            "üìä QUICK COMPARISON:\n",
            "  Top 4 keywords:  9,425 bibcodes\n",
            "  Top 15 keywords: 3,402 bibcodes\n",
            "  Overlap between searches: 3,402 bibcodes\n",
            "  36.1% of top-4 results also found in top-15 search\n"
          ]
        }
      ],
      "source": [
        "# Search with TOP 4 KEYWORDS for comparison\n",
        "print(\"üéØ BIBCODE SEARCH WITH TOP 4 KEYWORDS - ALL RESULTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "CHOSEN_KEYWORDS_4 = top_4_keywords  # Using top 4 keywords\n",
        "CHOSEN_SEARCH_FIELD_4 = \"full\"      # Full text search\n",
        "\n",
        "print(f\"Keywords: {CHOSEN_KEYWORDS_4}\")\n",
        "print(f\"Search field: {CHOSEN_SEARCH_FIELD_4}\")\n",
        "print(f\"Getting ALL bibcodes (automatic pagination)\")\n",
        "\n",
        "# Search for ALL bibcodes with top 4 keywords\n",
        "found_bibcodes_4 = search_all_bibcodes(CHOSEN_KEYWORDS_4, CHOSEN_SEARCH_FIELD_4)\n",
        "\n",
        "print(f\"\\nüìã TOP 4 KEYWORDS SEARCH RESULTS:\")\n",
        "print(f\"Total bibcodes retrieved: {len(found_bibcodes_4):,}\")\n",
        "\n",
        "if found_bibcodes_4:\n",
        "    print(f\"\\nFirst 10 bibcodes:\")\n",
        "    for i, bibcode in enumerate(found_bibcodes_4[:10]):\n",
        "        print(f\"  {i+1:2d}. {bibcode}\")\n",
        "    \n",
        "    if len(found_bibcodes_4) > 10:\n",
        "        print(f\"  ... and {len(found_bibcodes_4) - 10:,} more\")\n",
        "\n",
        "# Quick comparison with 15-keyword results\n",
        "if 'found_bibcodes' in locals() and found_bibcodes:\n",
        "    print(f\"\\nüìä QUICK COMPARISON:\")\n",
        "    print(f\"  Top 4 keywords:  {len(found_bibcodes_4):,} bibcodes\")\n",
        "    print(f\"  Top 15 keywords: {len(found_bibcodes):,} bibcodes\")\n",
        "    \n",
        "    # Check overlap between the two searches\n",
        "    if found_bibcodes_4 and found_bibcodes:\n",
        "        overlap_4_15 = set(found_bibcodes_4).intersection(set(found_bibcodes))\n",
        "        print(f\"  Overlap between searches: {len(overlap_4_15):,} bibcodes\")\n",
        "        \n",
        "        if len(found_bibcodes_4) > 0:\n",
        "            overlap_pct = (len(overlap_4_15) / len(found_bibcodes_4)) * 100\n",
        "            print(f\"  {overlap_pct:.1f}% of top-4 results also found in top-15 search\")\n",
        "else:\n",
        "    print(f\"\\nüí° Run the 15-keyword search first to enable comparison\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Compare with WUMaCat Bibcodes\n",
        "\n",
        "Compare the found bibcodes with the unique bibcodes from WUMaCat.csv to see overlaps and differences.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÇ LOADING WUMACAT BIBCODES\n",
            "==================================================\n",
            "‚úÖ Loaded WUMaCat.csv\n",
            "üìä Total unique bibcodes in WUMaCat: 424\n",
            "\n",
            "First 10 WUMaCat bibcodes:\n",
            "   1. 2015AJ....150....9X\n",
            "   2. 2015NewA...36..100G\n",
            "   3. 2011PASP..123..895Y\n",
            "   4. 2009Ap&SS.321...19L\n",
            "   5. 2018NewA...61....1K\n",
            "   6. 2011A&A...525A..66D\n",
            "   7. 2013NewA...23...59B\n",
            "   8. 2009Ap&SS.321..209H\n",
            "   9. 2016AJ....152..219S\n",
            "  10. 2010RAA....10..569H\n",
            "\n",
            "üîç BIBCODE COMPARISON\n",
            "==================================================\n",
            "üìä COMPARISON RESULTS:\n",
            "  Found in ADS search:     9,425 bibcodes\n",
            "  Found in WUMaCat:       424 bibcodes\n",
            "  Overlap (both):         366 bibcodes\n",
            "  ADS only (new):         9,059 bibcodes\n",
            "  WUMaCat only (missing): 58 bibcodes\n",
            "\n",
            "üìà OVERLAP STATISTICS:\n",
            "  3.9% of ADS results are already in WUMaCat\n",
            "  86.3% of WUMaCat papers found in ADS search\n",
            "\n",
            "‚úÖ OVERLAPPING BIBCODES (first 5):\n",
            "  1. 2015AJ....150....9X\n",
            "  2. 2015NewA...36..100G\n",
            "  3. 2011PASP..123..895Y\n",
            "  4. 2018NewA...61....1K\n",
            "  5. 2009Ap&SS.321...19L\n",
            "\n",
            "üÜï NEW BIBCODES FROM ADS (first 5):\n",
            "  1. 2012NewA...17...46U\n",
            "  2. 2000Ap&SS.273..257Q\n",
            "  3. 1977ApJ...211..853L\n",
            "  4. 2021JAVSO..49..170A\n",
            "  5. 2024arXiv241022427C\n",
            "\n",
            "üíæ Comparison results saved to: ../data/bibcode_comparison.json\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load WUMaCat.csv and extract bibcodes\n",
        "print(\"üìÇ LOADING WUMACAT BIBCODES\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "wumacat_file = '../data/WUMaCat.csv'\n",
        "if os.path.exists(wumacat_file):\n",
        "    # Read the CSV file\n",
        "    wumacat_df = pd.read_csv(wumacat_file)\n",
        "    \n",
        "    # Extract unique bibcodes (column 'Bibcode')\n",
        "    wumacat_bibcodes = set(wumacat_df['Bibcode'].dropna().unique())\n",
        "    \n",
        "    print(f\"‚úÖ Loaded WUMaCat.csv\")\n",
        "    print(f\"üìä Total unique bibcodes in WUMaCat: {len(wumacat_bibcodes)}\")\n",
        "    \n",
        "    # Show sample WUMaCat bibcodes\n",
        "    print(f\"\\nFirst 10 WUMaCat bibcodes:\")\n",
        "    for i, bibcode in enumerate(list(wumacat_bibcodes)[:10]):\n",
        "        print(f\"  {i+1:2d}. {bibcode}\")\n",
        "else:\n",
        "    print(f\"‚ùå WUMaCat file not found: {wumacat_file}\")\n",
        "    wumacat_bibcodes = set()\n",
        "\n",
        "# Compare the bibcodes\n",
        "print(f\"\\nüîç BIBCODE COMPARISON\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "if found_bibcodes_4 and wumacat_bibcodes:\n",
        "    # Convert found_bibcodes to set for comparison\n",
        "    found_bibcodes_set = set(found_bibcodes_4)\n",
        "    \n",
        "    # Find overlaps and differences\n",
        "    overlap = found_bibcodes_set.intersection(wumacat_bibcodes)\n",
        "    ads_only = found_bibcodes_set - wumacat_bibcodes\n",
        "    wumacat_only = wumacat_bibcodes - found_bibcodes_set\n",
        "    \n",
        "    print(f\"üìä COMPARISON RESULTS:\")\n",
        "    print(f\"  Found in ADS search:     {len(found_bibcodes_set):,} bibcodes\")\n",
        "    print(f\"  Found in WUMaCat:       {len(wumacat_bibcodes):,} bibcodes\")\n",
        "    print(f\"  Overlap (both):         {len(overlap):,} bibcodes\")\n",
        "    print(f\"  ADS only (new):         {len(ads_only):,} bibcodes\")\n",
        "    print(f\"  WUMaCat only (missing): {len(wumacat_only):,} bibcodes\")\n",
        "    \n",
        "    # Calculate percentages\n",
        "    if len(found_bibcodes_set) > 0:\n",
        "        overlap_pct_ads = (len(overlap) / len(found_bibcodes_set)) * 100\n",
        "        print(f\"\\nüìà OVERLAP STATISTICS:\")\n",
        "        print(f\"  {overlap_pct_ads:.1f}% of ADS results are already in WUMaCat\")\n",
        "    \n",
        "    if len(wumacat_bibcodes) > 0:\n",
        "        overlap_pct_wumacat = (len(overlap) / len(wumacat_bibcodes)) * 100\n",
        "        print(f\"  {overlap_pct_wumacat:.1f}% of WUMaCat papers found in ADS search\")\n",
        "    \n",
        "    # Show samples\n",
        "    if overlap:\n",
        "        print(f\"\\n‚úÖ OVERLAPPING BIBCODES (first 5):\")\n",
        "        for i, bibcode in enumerate(list(overlap)[:5]):\n",
        "            print(f\"  {i+1}. {bibcode}\")\n",
        "    \n",
        "    if ads_only:\n",
        "        print(f\"\\nüÜï NEW BIBCODES FROM ADS (first 5):\")\n",
        "        for i, bibcode in enumerate(list(ads_only)[:5]):\n",
        "            print(f\"  {i+1}. {bibcode}\")\n",
        "    \n",
        "    # Save results for further analysis\n",
        "    comparison_results = {\n",
        "        'ads_bibcodes': list(found_bibcodes_set),\n",
        "        'wumacat_bibcodes': list(wumacat_bibcodes),\n",
        "        'overlap': list(overlap),\n",
        "        'ads_only': list(ads_only),\n",
        "        'wumacat_only': list(wumacat_only)\n",
        "    }\n",
        "    \n",
        "    # Save to JSON file\n",
        "    output_file = '../data/bibcode_comparison.json'\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(comparison_results, f, indent=2, ensure_ascii=False)\n",
        "    \n",
        "    print(f\"\\nüíæ Comparison results saved to: {output_file}\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùå Cannot perform comparison - missing bibcode data\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary: Comparison of Search Strategies\n",
        "\n",
        "This section provides a summary comparison of the different keyword strategies used.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä COMPREHENSIVE SEARCH STRATEGY COMPARISON\n",
            "======================================================================\n",
            "üóÇÔ∏è  WUMaCat baseline:     424 bibcodes\n",
            "\n",
            "üîç ADS SEARCH RESULTS:\n",
            "   15 keywords    : 3,402 bibcodes\n",
            "   4 keywords     : 9,425 bibcodes\n",
            "\n",
            "üîó SEARCH OVERLAP ANALYSIS:\n",
            "   Overlap between 15 keywords and 4 keywords: 3,402 bibcodes\n",
            "   100.0% of 15 keywords results found in 4 keywords search\n",
            "   36.1% of 4 keywords results found in 15 keywords search\n",
            "\n",
            "üìã WUMaCat OVERLAP ANALYSIS:\n",
            "   15 keywords    : 316 overlap ( 9.3%), 3,086 new discoveries\n",
            "   4 keywords     : 366 overlap ( 3.9%), 9,059 new discoveries\n",
            "\n",
            "üí° RECOMMENDATIONS:\n",
            "   ‚Ä¢ 15 keywords search: More focused, 3,402 results\n",
            "   ‚Ä¢ 4 keywords search: Broader coverage, 9,425 results\n",
            "   ‚Ä¢ Consider your research goals: precision vs. completeness\n",
            "\n",
            "üíæ Search strategy summary saved to: ../data/search_strategy_summary.json\n"
          ]
        }
      ],
      "source": [
        "# SUMMARY: Compare all search strategies\n",
        "print(\"üìä COMPREHENSIVE SEARCH STRATEGY COMPARISON\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Check what searches were performed\n",
        "searches_performed = []\n",
        "if 'found_bibcodes' in locals() and found_bibcodes:\n",
        "    searches_performed.append(('15 keywords', len(found_bibcodes), found_bibcodes))\n",
        "    \n",
        "if 'found_bibcodes_4' in locals() and found_bibcodes_4:\n",
        "    searches_performed.append(('4 keywords', len(found_bibcodes_4), found_bibcodes_4))\n",
        "\n",
        "if 'wumacat_bibcodes' in locals() and wumacat_bibcodes:\n",
        "    wumacat_count = len(wumacat_bibcodes)\n",
        "    print(f\"üóÇÔ∏è  WUMaCat baseline:     {wumacat_count:,} bibcodes\")\n",
        "\n",
        "if searches_performed:\n",
        "    print(f\"\\nüîç ADS SEARCH RESULTS:\")\n",
        "    for name, count, bibcodes in searches_performed:\n",
        "        print(f\"   {name:15s}: {count:,} bibcodes\")\n",
        "    \n",
        "    # Compare overlaps between different searches\n",
        "    if len(searches_performed) >= 2:\n",
        "        search1_name, search1_count, search1_bibcodes = searches_performed[0]\n",
        "        search2_name, search2_count, search2_bibcodes = searches_performed[1]\n",
        "        \n",
        "        overlap_searches = set(search1_bibcodes).intersection(set(search2_bibcodes))\n",
        "        \n",
        "        print(f\"\\nüîó SEARCH OVERLAP ANALYSIS:\")\n",
        "        print(f\"   Overlap between {search1_name} and {search2_name}: {len(overlap_searches):,} bibcodes\")\n",
        "        \n",
        "        if search1_count > 0 and search2_count > 0:\n",
        "            pct1 = (len(overlap_searches) / search1_count) * 100\n",
        "            pct2 = (len(overlap_searches) / search2_count) * 100\n",
        "            print(f\"   {pct1:.1f}% of {search1_name} results found in {search2_name} search\")\n",
        "            print(f\"   {pct2:.1f}% of {search2_name} results found in {search1_name} search\")\n",
        "    \n",
        "    # WUMaCat comparisons\n",
        "    if 'wumacat_bibcodes' in locals():\n",
        "        print(f\"\\nüìã WUMaCat OVERLAP ANALYSIS:\")\n",
        "        for name, count, bibcodes in searches_performed:\n",
        "            overlap_wuma = set(bibcodes).intersection(wumacat_bibcodes)\n",
        "            new_discoveries = set(bibcodes) - wumacat_bibcodes\n",
        "            \n",
        "            if count > 0:\n",
        "                overlap_pct = (len(overlap_wuma) / count) * 100\n",
        "                print(f\"   {name:15s}: {len(overlap_wuma):3d} overlap ({overlap_pct:4.1f}%), {len(new_discoveries):,} new discoveries\")\n",
        "    \n",
        "    # Final recommendations\n",
        "    print(f\"\\nüí° RECOMMENDATIONS:\")\n",
        "    if len(searches_performed) >= 2:\n",
        "        smaller_search = min(searches_performed, key=lambda x: x[1])\n",
        "        larger_search = max(searches_performed, key=lambda x: x[1])\n",
        "        \n",
        "        print(f\"   ‚Ä¢ {smaller_search[0]} search: More focused, {smaller_search[1]:,} results\")\n",
        "        print(f\"   ‚Ä¢ {larger_search[0]} search: Broader coverage, {larger_search[1]:,} results\")\n",
        "        print(f\"   ‚Ä¢ Consider your research goals: precision vs. completeness\")\n",
        "    \n",
        "    # Save comprehensive summary\n",
        "    summary_data = {\n",
        "        'search_strategies': {\n",
        "            name: {\n",
        "                'total_bibcodes': count,\n",
        "                'sample_bibcodes': bibcodes[:10] if bibcodes else []\n",
        "            } for name, count, bibcodes in searches_performed\n",
        "        },\n",
        "        'wumacat_baseline': wumacat_count if 'wumacat_bibcodes' in locals() else 0\n",
        "    }\n",
        "    \n",
        "    summary_file = '../data/search_strategy_summary.json'\n",
        "    with open(summary_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(summary_data, f, indent=2, ensure_ascii=False)\n",
        "    \n",
        "    print(f\"\\nüíæ Search strategy summary saved to: {summary_file}\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùå No search results available for comparison\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
